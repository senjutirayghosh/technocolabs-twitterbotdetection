{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete\n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "#from keras.preprocessing import text, sequence\n",
    "#from keras import layers, models, optimizers\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>['giuliani', 'tax', 'report', 'proves', 'trump...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>['trump', 'temp', 'crab', 'orchard', 'ky', 'f'...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>['iconic', 'charcoaler', 'hamburger', 'brand']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>['new', 'audio', 'clinton', 'refers', 'sanders...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>['make', 'money', 'sleeping', 'please', 'syeck...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  label\n",
       "0  ['giuliani', 'tax', 'report', 'proves', 'trump...      0\n",
       "1  ['trump', 'temp', 'crab', 'orchard', 'ky', 'f'...      0\n",
       "2     ['iconic', 'charcoaler', 'hamburger', 'brand']      1\n",
       "3  ['new', 'audio', 'clinton', 'refers', 'sanders...      0\n",
       "4  ['make', 'money', 'sleeping', 'please', 'syeck...      1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dataset preparation\n",
    "tweets = pd.read_csv(\"Final dataset 2.csv\")\n",
    "tweets = tweets[['content', 'label']]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['tweet'] = tweets['content']\n",
    "df['label'] = tweets['label']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training set:  13184\n",
      "Size of the testing set:  4395\n"
     ]
    }
   ],
   "source": [
    "#Splitting the dataset into training and testing datasets\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(df['tweet'], df['label'])\n",
    "\n",
    "print(\"Size of the training set: \", len(train_x))\n",
    "print(\"Size of the testing set: \", len(test_x))\n",
    "\n",
    "#Label encoding the target variable\n",
    "enc = preprocessing.LabelEncoder()\n",
    "train_y = enc.fit_transform(train_y)\n",
    "test_y = enc.fit_transform(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binary Count Vector as features\n",
    "binary_count_vect = CountVectorizer(analyzer = 'word', token_pattern = r'\\w{1,}', binary = True)\n",
    "binary_count_vect.fit(df['tweet'])\n",
    "\n",
    "#Creating a count vectorizer object\n",
    "count_vect = CountVectorizer(analyzer = 'word', token_pattern = r'\\w{1,}')\n",
    "count_vect.fit(df['tweet'])\n",
    "\n",
    "#Transforming the training and validation data using count vectorizer object\n",
    "xtrain_binary_count = binary_count_vect.transform(train_x)\n",
    "xtest_binary_count = binary_count_vect.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count Vector as features\n",
    "\n",
    "\n",
    "#Creating a count vectorizer object\n",
    "count_vect = CountVectorizer(analyzer = 'word', token_pattern = r'\\w{1,}')\n",
    "count_vect.fit(df['tweet'])\n",
    "\n",
    "#Transforming the training and validation data using count vectorizer object\n",
    "xtrain_count = count_vect.transform(train_x)\n",
    "xtest_count = count_vect.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF as features\n",
    "\n",
    "\n",
    "#Word-level TF-IDF\n",
    "tfidf_vect_word = TfidfVectorizer(analyzer = 'word', token_pattern = r'\\w{1,}', max_features = 5000)\n",
    "tfidf_vect_word.fit(df['tweet'])\n",
    "xtrain_tfidf_word = tfidf_vect_word.transform(train_x)\n",
    "xtest_tfidf_word = tfidf_vect_word.transform(test_x)\n",
    "\n",
    "#Character-level TF-IDF\n",
    "tfidf_vect_char = TfidfVectorizer(analyzer = 'char', token_pattern = r'\\w{1,}', ngram_range = (2,3), max_features = 5000)\n",
    "tfidf_vect_char.fit(df['tweet'])\n",
    "xtrain_tfidf_char = tfidf_vect_char.transform(train_x)\n",
    "xtest_tfidf_char = tfidf_vect_char.transform(test_x)\n",
    "\n",
    "#n-gram-level TF-IDF\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer = 'word', token_pattern = r'\\w{1,}', ngram_range = (2,3), max_features = 5000)\n",
    "tfidf_vect_ngram.fit(df['tweet'])\n",
    "xtrain_tfidf_ngram = tfidf_vect_ngram.transform(train_x)\n",
    "xtest_tfidf_ngram = tfidf_vect_ngram.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Character-level TF-IDF:  0.8714448236632537\n"
     ]
    }
   ],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_test):\n",
    "    #Fitting the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    #Predicting the labels on testing dataset\n",
    "    train_pred = classifier.predict(feature_vector_train)\n",
    "    test_pred = classifier.predict(feature_vector_test)\n",
    "    \n",
    "    train_acc = metrics.accuracy_score(train_pred, train_y)\n",
    "    test_acc = metrics.accuracy_score(test_pred, test_y)\n",
    "    cm = metrics.confusion_matrix(test_y, test_pred)\n",
    "    \"\"\"\n",
    "    print(\"Training accuracy: \", train_acc)\n",
    "    print(\"Testing accuracy: \", test_acc)\n",
    "    print(\"Confusion matrix: \", cm)\n",
    "    \"\"\"\n",
    "    return test_acc\n",
    "\n",
    "#Naive Bayes on Character-level TF-IDF\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_char, train_y, xtest_tfidf_char)\n",
    "print(\"NB, Character-level TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Binary Count Vectors:  0.8687144482366326\n",
      "NB, Count Vectors:  0.8696245733788396\n",
      "NB, Word-level TF-IDF:  0.855745164960182\n",
      "NB, Character-level TF-IDF:  0.8714448236632537\n",
      "NB, n-gram TF-IDF:  0.7754266211604095\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes on Binary Count Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_binary_count, train_y, xtest_binary_count)\n",
    "print(\"NB, Binary Count Vectors: \", accuracy)\n",
    "\n",
    "#Naive Bayes on Count Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xtest_count)\n",
    "print(\"NB, Count Vectors: \", accuracy)\n",
    "\n",
    "#Naive Bayes on Word-level TF-IDF\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_word, train_y, xtest_tfidf_word)\n",
    "print(\"NB, Word-level TF-IDF: \", accuracy)\n",
    "\n",
    "#Naive Bayes on Character-level TF-IDF\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_char, train_y, xtest_tfidf_char)\n",
    "print(\"NB, Character-level TF-IDF: \", accuracy)\n",
    "\n",
    "#Naive Bayes on n-gram TF-IDF\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xtest_tfidf_ngram)\n",
    "print(\"NB, n-gram TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Binary Count Vectors:  0.8994311717861206\n",
      "LR, Count Vectors:  0.8987485779294653\n",
      "LR, Word-level TF-IDF:  0.8937428896473265\n",
      "LR, Character-level TF-IDF:  0.8939704209328783\n",
      "LR, n-gram TF-IDF:  0.8061433447098976\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression on Binary Count Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_binary_count, train_y, xtest_binary_count)\n",
    "print(\"LR, Binary Count Vectors: \", accuracy)\n",
    "\n",
    "#Logistic Regression on Count Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xtest_count)\n",
    "print(\"LR, Count Vectors: \", accuracy)\n",
    "\n",
    "#Logistic Regression on Word-level TF-IDF\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_word, train_y, xtest_tfidf_word)\n",
    "print(\"LR, Word-level TF-IDF: \", accuracy)\n",
    "\n",
    "#Logistic Regression on Character-level TF-IDF\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_char, train_y, xtest_tfidf_char)\n",
    "print(\"LR, Character-level TF-IDF: \", accuracy)\n",
    "\n",
    "#Logistic Regression on n-gram TF-IDF\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xtest_tfidf_ngram)\n",
    "print(\"LR, n-gram TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, Binary Count Vectors:  0.8234357224118316\n",
      "SVM, Count Vectors:  0.825938566552901\n",
      "SVM, Word-level TF-IDF:  0.8573378839590444\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-3e8c2ded5de3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#Support Vector Machine on Character-level TF-IDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'linear'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxtrain_tfidf_char\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxtest_tfidf_char\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SVM, Character-level TF-IDF: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-e2852dabe9f0>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(classifier, feature_vector_train, label, feature_vector_test)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#Predicting the labels on testing dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtrain_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_vector_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mtest_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_vector_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    572\u001b[0m             \u001b[0mClass\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m         \"\"\"\n\u001b[1;32m--> 574\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    575\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    322\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_dense_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_sparse_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    367\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshrinking\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobability\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_support_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 369\u001b[1;33m             self.probA_, self.probB_)\n\u001b[0m\u001b[0;32m    370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_compute_kernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Support Vector Machine\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#Support Vector Machine on Binary Count Vectors\n",
    "accuracy = train_model(svm.SVC(kernel = 'linear', C = 10), xtrain_binary_count, train_y, xtest_binary_count)\n",
    "print(\"SVM, Binary Count Vectors: \", accuracy)\n",
    "\n",
    "#Support Vector Machine on Count Vectors\n",
    "accuracy = train_model(svm.SVC(kernel = 'linear', C = 10), xtrain_count, train_y, xtest_count)\n",
    "print(\"SVM, Count Vectors: \", accuracy)\n",
    "\n",
    "#Support Vector Machine on Word-level TF-IDF\n",
    "accuracy = train_model(svm.SVC(kernel = 'linear', C = 10), xtrain_tfidf_word, train_y, xtest_tfidf_word)\n",
    "print(\"SVM, Word-level TF-IDF: \", accuracy)\n",
    "\n",
    "#Support Vector Machine on Character-level TF-IDF\n",
    "accuracy = train_model(svm.SVC(kernel = 'linear', C = 10), xtrain_tfidf_char, train_y, xtest_tfidf_char)\n",
    "print(\"SVM, Character-level TF-IDF: \", accuracy)\n",
    "\n",
    "#Support Vector Machine on n-gram TF-IDF\n",
    "accuracy = train_model(svm.SVC(kernel = 'linear', C = 10), xtrain_tfidf_ngram, train_y, xtest_tfidf_ngram)\n",
    "print(\"SVM, n-gram TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "\n",
    "#Random Forest on Binary Count Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_binary_count, train_y, xtest_binary_count)\n",
    "print(\"RF, Binary Count Vectors: \", accuracy)\n",
    "\n",
    "#Support Vector Machine on Count Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xtest_count)\n",
    "print(\"RF, Count Vectors: \", accuracy)\n",
    "\n",
    "#Support Vector Machine on Word-level TF-IDF\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf_word, train_y, xtest_tfidf_word)\n",
    "print(\"RF, Word-level TF-IDF: \", accuracy)\n",
    "\n",
    "#Support Vector Machine on Character-level TF-IDF\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf_char, train_y, xtest_tfidf_char)\n",
    "print(\"RF, Character-level TF-IDF: \", accuracy)\n",
    "\n",
    "#Support Vector Machine on n-gram TF-IDF\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf_ngram, train_y, xtest_tfidf_ngram)\n",
    "print(\"RF, n-gram TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
